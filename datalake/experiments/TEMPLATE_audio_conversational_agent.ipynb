{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a8a1c5-c3e6-4a9e-96c0-c3cca62884f3",
   "metadata": {},
   "source": [
    "# Audio RAG Chatbot with Langchain and AssemblyAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188c9a1-4dc5-4792-baa9-e2358a30d8d4",
   "metadata": {},
   "source": [
    "In this notebook we create a chatbot with functionality to transcribe audio files and use them to answer the questions from the user. \n",
    "The chatbot is based on langchain agent with Multi-Query retriever tool for RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510f96b-d3ff-45f2-bec4-9eff274ccfc3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ef3f8ca-eb80-4b5a-8934-744e43433a0b",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!pip install \\\n",
    "    python-dotenv==1.0.0 \\\n",
    "    openai==0.28.1 \\\n",
    "    langchain==0.0.316 \\\n",
    "    tiktoken==0.5.1 \\\n",
    "    gradio==4.5.0 \\\n",
    "    chromadb==0.4.15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34564aeb-b6c0-42b0-93a4-46156f255d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System related imports\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# OpenAI related imports\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# AssemblyAI and Document storage related imports\n",
    "import tiktoken\n",
    "import assemblyai as aai\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import AssemblyAIAudioTranscriptLoader\n",
    "\n",
    "# Agent imports\n",
    "import logging\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "# Chat UI\n",
    "import gradio as gr\n",
    "from langchain.schema import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add021b-201f-47ad-b4e3-44ba810d02c2",
   "metadata": {},
   "source": [
    "# Enviromental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07106937-17ff-4a22-8d27-5c79a8a83591",
   "metadata": {},
   "source": [
    "Set up the APIs keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3270c64b-5168-4085-b3d3-07a6ad1a2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce25c860-8b01-4f68-954f-1962fe17887c",
   "metadata": {},
   "source": [
    "# Transribing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1c5455-c41e-4852-9af4-b640593451c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = [\n",
    "    \"https://storage.googleapis.com/aai-web-samples/langchain_retrieval_webinar.opus\",\n",
    "    \"https://storage.googleapis.com/aai-web-samples/langchain_agents_webinar.opus\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0741b1-0262-4c0f-b3d8-d188f43c4e01",
   "metadata": {},
   "source": [
    "Use AssemblyAI API to aggregate the audio and get the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c2ed5b-e3c3-412c-aab2-973fac60f432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing https://storage.googleapis.com/aai-web-samples/langchain_retrieval_webinar.opus\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please provide an API key via the ASSEMBLYAI_API_KEY environment variable or the global settings.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m         transcripts\u001b[38;5;241m.\u001b[39mappend(AssemblyAIAudioTranscriptLoader(file_path\u001b[38;5;241m=\u001b[39murl)\u001b[38;5;241m.\u001b[39mload()[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transcripts\n\u001b[1;32m----> 8\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mURLs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# list of documents\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m URLs:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranscribing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     transcripts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mAssemblyAIAudioTranscriptLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mload()[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transcripts\n",
      "File \u001b[1;32mc:\\Users\\logis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\document_loaders\\assemblyai.py:74\u001b[0m, in \u001b[0;36mAssemblyAIAudioTranscriptLoader.__init__\u001b[1;34m(self, file_path, transcript_format, config, api_key)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m file_path\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscript_format \u001b[38;5;241m=\u001b[39m transcript_format\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscriber \u001b[38;5;241m=\u001b[39m \u001b[43massemblyai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTranscriber\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\logis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\assemblyai\\transcriber.py:820\u001b[0m, in \u001b[0;36mTranscriber.__init__\u001b[1;34m(self, client, config, max_workers)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m     max_workers: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    795\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;124;03m    Initializes the `Transcriber` with the given parameters.\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl \u001b[38;5;241m=\u001b[39m _TranscriberImpl(\n\u001b[0;32m    823\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m    824\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig \u001b[38;5;129;01mor\u001b[39;00m types\u001b[38;5;241m.\u001b[39mTranscriptionConfig(),\n\u001b[0;32m    825\u001b[0m     )\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m max_workers:\n",
      "File \u001b[1;32mc:\\Users\\logis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\assemblyai\\client.py:76\u001b[0m, in \u001b[0;36mClient.get_default\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_default\u001b[38;5;241m.\u001b[39msettings \u001b[38;5;241m!=\u001b[39m default_settings:\n\u001b[1;32m---> 76\u001b[0m             \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_default \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_default\n",
      "File \u001b[1;32mc:\\Users\\logis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\assemblyai\\client.py:29\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39mapi_key:\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide an API key via the ASSEMBLYAI_API_KEY environment variable or the global settings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m     )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(\n\u001b[0;32m     34\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mbase_url,\n\u001b[0;32m     35\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mhttp_timeout,\n\u001b[0;32m     39\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Please provide an API key via the ASSEMBLYAI_API_KEY environment variable or the global settings."
     ]
    }
   ],
   "source": [
    "def transcribe_audio(url):\n",
    "    transcripts = []\n",
    "    for url in URLs:\n",
    "        print(f'Transcribing {url}')\n",
    "        transcripts.append(AssemblyAIAudioTranscriptLoader(file_path=url).load()[0])\n",
    "    return transcripts\n",
    "\n",
    "docs = transcribe_audio(URLs) # list of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0bfab-2a27-4d60-8907-14951972b7ab",
   "metadata": {},
   "source": [
    "# Vector storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354050ef-7da7-47e9-a9c6-27808b363d3b",
   "metadata": {},
   "source": [
    "Split the documents to store the embedded splits in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b312b-aa51-412e-85f4-715b227c51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb5d39-5a44-4663-bd29-175f0fd52283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify metadata because some AssemblyAI returned metadata is not in a compatible form for the Chroma db\n",
    "for split in splits:\n",
    "    split.metadata = {\"audio_url\": split.metadata[\"audio_url\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb50bca-9dd5-47a8-839b-1e21a3614c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old database files if any\n",
    "# !rm -rf ./docs/chroma "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e12915-9648-44cc-a80f-974c129eae5d",
   "metadata": {},
   "source": [
    "Creating vectordb with embedded splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1b7fa-7fb0-4309-9092-5826a67190ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "persist_directory = 'docs/chroma/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a85f7b-e3d3-4966-a536-cd764902bbf4",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9306ddb-3b03-4b5c-a23f-63ef560f5185",
   "metadata": {},
   "source": [
    "We have to convert the retriever to the tool so it can be used by an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d9a43-9bd8-44eb-838b-5f51ca1a8274",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Output parser will split the LLM result into a list of queries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLineList\u001b[39;00m(\u001b[43mBaseModel\u001b[49m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# \"lines\" is the key (attribute name) of the parsed output\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     lines: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLines of text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLineListOutputParser\u001b[39;00m(PydanticOutputParser):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "multi_query_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
    "\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectordb.as_retriever(), llm_chain=multi_query_chain, parser_key=\"lines\"\n",
    ")\n",
    "\n",
    "search_related_documents = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"RAG\",\n",
    "    \"Retrieves context related to the question from the audio provided by the user.\",\n",
    ")\n",
    "\n",
    "tools = [search_related_documents]\n",
    "functions = [format_tool_to_openai_function(f) for f in tools] # convert tools to OpenAI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_related_documents = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"RAG\",\n",
    "    \"Retrieves context related to the question from the audio provided by the user.\",\n",
    ")\n",
    "\n",
    "tools = [search_related_documents]\n",
    "functions = [format_tool_to_openai_function(f) for f in tools] # convert tools to OpenAI functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293525ac-5cb9-426c-b9ac-727d763ba037",
   "metadata": {},
   "source": [
    "## Agent chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4563fa-c412-4d95-ab82-ee62a08fe2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful assistant. The user provided an audio as context to this chat. You can use it to retrieve context to answer the questions.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630f996-1016-4533-9887-7660285f4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9fbbab-732a-4b39-903c-8b270e1ac9b2",
   "metadata": {},
   "source": [
    "## Agent executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8bde4c-4b57-4b8f-bfa6-fb5b6a7e467c",
   "metadata": {},
   "source": [
    "Create the agent executor that will run the agent chain until the stop condition is met (in our case the system defines the stop on its own)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd736bc2-185b-46c7-8049-f32ba2566893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define memory type to store the conversation history\n",
    "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256219a3-ed6e-4ad8-98a4-62b643d00c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ed099-093a-4301-b0bd-e6907a41d034",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848a164-649a-474a-a3fb-63f3f0a3ec74",
   "metadata": {},
   "source": [
    "Let's try to make some questions not related to the document and check if it is able to store the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6fe333-fb6a-4981-a3ef-2ebe7125a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello Logis! Nice to meet you too. How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi! Nice to meet you. My name is Logis.',\n",
       " 'chat_history': [HumanMessage(content='Hi! Nice to meet you. My name is Logis.'),\n",
       "  AIMessage(content='Hello Logis! Nice to meet you too. How can I assist you today?')],\n",
       " 'output': 'Hello Logis! Nice to meet you too. How can I assist you today?'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Hi! Nice to meet you. My name is Logis.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563cf39-b209-44ff-9f85-46de4f844fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mYour name is Logis.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name?',\n",
       " 'chat_history': [HumanMessage(content='Hi! Nice to meet you. My name is Logis.'),\n",
       "  AIMessage(content='Hello Logis! Nice to meet you too. How can I assist you today?'),\n",
       "  HumanMessage(content='What is my name?'),\n",
       "  AIMessage(content='Your name is Logis.')],\n",
       " 'output': 'Your name is Logis.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db293ce6-6911-46f0-a486-d13fbc86a1ea",
   "metadata": {},
   "source": [
    "Everything works well, what about retriving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7e529-273d-4f16-9eae-d3057b880c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `RAG` with `audio`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are some applications of audio technology?', '2. How does audio compression work?', '3. Can you recommend any audio editing software?', '4. What are the different types of audio formats?', '5. How does audio streaming work?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[Document(page_content=\"of your application. Maybe that's search, maybe that is some generative component. Whatever it is, at least that's the way I think of maybe Harrison can maybe comment on this a little bit, but I think with the self improvement, I think you're right that finding ways to make that self improvement persist from one conversation to another is huge. And DSP seems like a promising way to do that. I think in terms of the retrieval and long term memory side for agents, the way it normally looks is that the amount of information that you're searching over is going to generally be smaller. Right? It's like the memory of a single agent. That's more like the scale of the amount of information a person consumes on the internet. So we're talking hundreds of documents, thousands of documents rather than millions. And then you also are maybe doing some annotation of that at the same time with the language model. So it's not just a document, it's a document with additional metadata, which is a pattern\", metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_retrieval_webinar.opus'}), Document(page_content=\"AGI. First of all, I want John Carmack in April of 2023 to update that. Like, how many do you think we have? But the second thing that he said, which I thought was really interesting and true, is that a lot of these ideas likely have already been created and explored academically. So there was a ton of just incredible agent work done even in the 60s, but especially in the mid and late eighty s, there were three architectures which are really important to note here atlantis, BB one and soar and there are others as well. But my suspicion is that something else that we could do to kind of push this along a lot more quickly is to go back and see, okay, well, a lot of this thinking has already been done for us, but they didn't have any of the engineering to be able to implement this. And a lot of the science to implement this. Okay, what do we have? It looks like we can now do these things in 2023. What? In Atlantis. BB one Soar and similar types of architectures, cognitive architectures.\", metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_agents_webinar.opus'}), Document(page_content=\"several times in each query. How can you make this efficient and how can you make sure this adapts to the actual queries people ask? So our solution for this is you deploy your agent, and in particular you deploy this tool in front of users. You gather a bunch of questions from them. These questions don't have labels, they don't have answers, but you could still gather them and save them and then you could just say, hey DST, can you please compile my program that we built here and use these user questions? And I'd like you to use in this pipeline maybe the default language model that you have, which might be configured to be T five large on GitHub. The default is ada. OpenAI. Ada. So we've been playing with this for a while, and if you compile a DSP program that does multi hop search into T five large, you match the quality of retrieval that you're getting from DaVinci 3.5, and you only lose a very small kind of margin in terms of the final answer accuracy. But you don't need to go to\", metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_retrieval_webinar.opus'}), Document(page_content='on trying to get models that generalize better, cober v one generalized better to new domains. Just by happenstance, it worked better. But Cober v two was purpose built for this by a more resilient denoise training strategy. Look, I refer you to the paper for the details of that. But we also built this residual compression technique so that each of the vectors in the Colbert storage are really small. So if you work with Colbert V two right now, even though we have more than ten times as many vectors as your off the shelf, by encoder, the vectors are actually taking no more space than a standard DPR retriever or these single vector retrievers, because each of our vectors could be encoded in as little as 20 bytes or something like that. So we have applied this in domain and out of domain. So the two tables on the right are out of domain tests, a whole bunch of them. Beer we also introduced this Latte, a play on the words there and across. I think it was something like you could count', metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_retrieval_webinar.opus'}), Document(page_content='on trying to get models that generalize better, cober v one generalized better to new domains. Just by happenstance, it worked better. But Cober v Two was purpose built for this by a more resilient denoise training strategy. Look, I refer you to the paper for the details of that. But we also built this residual compression technique so that each of the vectors in the Colbert storage are really small. So if you work with Colbert V two right now, even though we have more than ten times as many vectors as your off the shelf, by encoder, the vectors are actually taking no more space than a standard DPR retriever or these single vector retrievers, because each of our vectors could be encoded in as little as 20 bytes or something like that. So we have applied this in domain and out of domain. So the two tables on the right are out of domain tests, a whole bunch of them. Beer we also introduced this Latte, a play on the words there and across. I think it was something like you could count', metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_retrieval_webinar.opus'}), Document(page_content=\"this can raise accuracy on benchmarks like hotpot QA from around 30% in a zero shot setting to a 35% in a few shot setting that you did not write the prompts for. So it can teach itself how to generate queries and how to use the passages and how to control the flow, et cetera. And you could easily add extensions to the pipeline that keep praising this. A very simple like few line addition makes this 41% exact match. So another sort of magical features, and I'll conclude with this, is the compile primitive in DSP. So let's say that and a lot of people ask things related to this. Let's say that you have this program now, which is a bootstrapped, few shot react agent built in DSP as a tool that you can use in your language conversational pipeline. But it's kind of expensive because it's going to talk to GPT 3.5 or GPT Four or whatever it is, several times in each query. How can you make this efficient and how can you make sure this adapts to the actual queries people ask? So our solution\", metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_retrieval_webinar.opus'}), Document(page_content=\"So you can basically evaluate that. And what is the generation that is coming out of that? Is that hallucinated or is it not? Right, so I think having that separation because that means you might have solved the retrieval problem, but then you have more of a prompt problem, the language model, how you tune that so you can separate, how you spend the rest of the time improving it. I think that makes sense. Yeah. Sort of treating these as separate types of bugs. Separate like exactly. Okay, well, then, yeah, we'll hear again from Joe when we have our open Q A session at the end. But next up we have Omar. So let me pull you up. So Omar was going to share primarily about a specific technique and some models for the specific technique of multi vector retrieval. So, yeah, Omar, I think you can take it away. All right. Yeah. Thanks for hosting us, Harrison, and thanks for joining Charles and leading the discussion. Joe, I really enjoyed your talk. We've been in touch for a long time since\", metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_retrieval_webinar.opus'}), Document(page_content=\"kind of show you guys what I mean if I can share my screen here. Let me know if you can see my screen. There's a Q and A button right next to the chat. Should be on the right hand side of your screen. You can put questions there and vote on questions that you want to hear the speakers discuss. And then Matt, you might want to zoom in a good bit on that one. A little hard to see. I think Matt Lang chain agent capped out. It broke. Yeah, we must hit the 32K context limit. That's my guess. Yeah, that's what it was. He tried to stuff it too much. Yeah. While we see what's happening there, let's see, there's a question, highly voted question from the Q A that I'd love to hear. Johan, should you talk like we're starting to see sort of like patterns of how agents work both at the level of low level prompting techniques about thought observation, action, and then also sort of like control flow techniques and augmentation techniques. So like have a memory stream that you can access that's\", metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_agents_webinar.opus'}), Document(page_content=\"the system prompt. And I can just kind of show you guys what I mean if I can share my screen here. Let me know if you can see my screen. There's a Q and A button right next to the chat. Should be on the right hand side of your screen. You can put questions there and vote on questions that you want to hear the speakers discuss. And then, Matt, you might want to zoom in a good bit on that one. A little hard to see. I think Matt Lang chain agent capped out. It broke. Yeah, we must hit the 32K context limit. That's my guess. Yeah, that's what it was. He tried to stuff it too much. Yeah. While we see what's happening there, let's see, there's a question, highly voted question from the Q A that I'd love to hear. Johan, should you talk like we're starting to see sort of like patterns of how agents work, both at the level of low level prompting techniques about thought observation action, and then also sort of like control flow techniques and augmentation techniques. So, like, have a memory\", metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_agents_webinar.opus'})]\u001b[0m\u001b[32;1m\u001b[1;3mThe topics discussed in the audio include:\n",
      "1. Self-improvement and the use of DSP (Dialogue System Platform) for persistent self-improvement in conversations.\n",
      "2. Different architectures and cognitive frameworks for building agents, such as Atlantis, BB one, Soar, and others.\n",
      "3. Deploying agents and gathering user questions to compile a program using DSP.\n",
      "4. Techniques for efficient and adaptive querying, including multi-hop search and the use of language models like T5 and Ada.\n",
      "5. Compression techniques for reducing the storage space required for vectors in retrieval systems.\n",
      "6. Improving model generalization and accuracy through denoise training and residual compression.\n",
      "7. The use of DSP's compile primitive for efficient and adaptive conversation flow.\n",
      "8. Separating retrieval and prompt problems in language models.\n",
      "9. Techniques for multi-vector retrieval and memory access in agents.\n",
      "10. Limitations and challenges in agent development, such as context limits and system prompts.\n",
      "\n",
      "Please note that the provided audio contains multiple discussions, so these topics are a summary of the main points covered.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What topics are discussed in the audio?',\n",
       " 'chat_history': [HumanMessage(content='Hi! Nice to meet you. My name is Logis.'),\n",
       "  AIMessage(content='Hello Logis! Nice to meet you too. How can I assist you today?'),\n",
       "  HumanMessage(content='What is my name?'),\n",
       "  AIMessage(content='Your name is Logis.'),\n",
       "  HumanMessage(content='What topics are discussed in the audio?'),\n",
       "  AIMessage(content=\"The topics discussed in the audio include:\\n1. Self-improvement and the use of DSP (Dialogue System Platform) for persistent self-improvement in conversations.\\n2. Different architectures and cognitive frameworks for building agents, such as Atlantis, BB one, Soar, and others.\\n3. Deploying agents and gathering user questions to compile a program using DSP.\\n4. Techniques for efficient and adaptive querying, including multi-hop search and the use of language models like T5 and Ada.\\n5. Compression techniques for reducing the storage space required for vectors in retrieval systems.\\n6. Improving model generalization and accuracy through denoise training and residual compression.\\n7. The use of DSP's compile primitive for efficient and adaptive conversation flow.\\n8. Separating retrieval and prompt problems in language models.\\n9. Techniques for multi-vector retrieval and memory access in agents.\\n10. Limitations and challenges in agent development, such as context limits and system prompts.\\n\\nPlease note that the provided audio contains multiple discussions, so these topics are a summary of the main points covered.\")],\n",
       " 'output': \"The topics discussed in the audio include:\\n1. Self-improvement and the use of DSP (Dialogue System Platform) for persistent self-improvement in conversations.\\n2. Different architectures and cognitive frameworks for building agents, such as Atlantis, BB one, Soar, and others.\\n3. Deploying agents and gathering user questions to compile a program using DSP.\\n4. Techniques for efficient and adaptive querying, including multi-hop search and the use of language models like T5 and Ada.\\n5. Compression techniques for reducing the storage space required for vectors in retrieval systems.\\n6. Improving model generalization and accuracy through denoise training and residual compression.\\n7. The use of DSP's compile primitive for efficient and adaptive conversation flow.\\n8. Separating retrieval and prompt problems in language models.\\n9. Techniques for multi-vector retrieval and memory access in agents.\\n10. Limitations and challenges in agent development, such as context limits and system prompts.\\n\\nPlease note that the provided audio contains multiple discussions, so these topics are a summary of the main points covered.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What topics are discussed in the audio?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7b2307-da5b-4fd0-9fcb-fd2ba7b4d051",
   "metadata": {},
   "source": [
    "# Creating UI for the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a099e-a4fd-4013-aaa2-c730715dc538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `RAG` with `main topics from the webinars in bulletpoints`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key subjects covered in the webinars, presented in bullet point format?', '2. Can you provide a list of the main topics discussed in the webinars, using bullet points?', '3. In bullet point form, what are the primary themes addressed in the webinars?', '4. Could you summarize the main subjects covered in the webinars, using bullet points?', '5. What are the main discussion points from the webinars, presented as bullet points?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[Document(page_content=\"better task prioritization agent for something like baby AGI. And the reason I bring that up is I think there's a lot of relationship between the way we operate, the way we think our brains, and as we think about building these autonomous agents that there's a lot of cross learning that we should absolutely be talking about. I did want to bring one last major topic in our ten minutes that we have remaining here, which was the question of safety. A lot of people are asking about security, like narrowly considered, which is like prompt, is this thing going to pseudo RMRF my machine? Is this thing going to drain my PayPal account? And then there's also the broader question of what happens when we unleash a whole bunch of much more capable web scraping and API hitting bots. So I'd love to hear just from each sort of panel member in what they think the path is for assuaging those concerns that people have. So I'd like to start with Shinyu. Yeah, actually I'm writing something about this.\", metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_agents_webinar.opus'}), Document(page_content=\"build a much better task prioritization agent for something like baby AGI. And the reason I bring that up is I think there's a lot of relationship between the way we operate, the way we think our brains, and as we think about building these autonomous agents that there's a lot of cross learning that we should absolutely be talking about. I did want to bring one last major topic in our ten minutes that we have remaining here, which was the question of safety. A lot of people are asking about security, like narrowly considered, which is like prompt, is this thing going to pseudo RMRF my machine? Is this thing going to drain my PayPal account? And then there's also the broader question of what happens when we unleash a whole bunch of much more capable web scraping and API hitting bots. So I'd love to hear just from each sort of panel member in what they think the path is for assuaging those concerns that people have. So I'd like to start with Shinyu. Yeah, actually I'm writing something\", metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_agents_webinar.opus'}), Document(page_content=\"nother webinar just on those last three responses, but fortunately, we've run out of time in this webinar. There was one question about Langchain that was the highest upvoted one that I wanted to hear from Harrison on. But before I kick that one over to Harrison, I did want to thank our panelists both for being here and answering questions and engaging with each other and with the audience, and also just generally for doing really incredible work and putting it out there for everybody else to learn from. Not everybody does that. Lots of people like to jealously hide their secrets. And so getting out there, putting things that other people can build on, it's something that we all, like everybody else in the community, really appreciates. So thanks to Matt, Johe and Shunyu for everything. And thank you, Charles, for moderating this. You're always the oh, thanks. It turns out that iteratively improving the criticism is actually the easiest part. The initial generation is the hard part.\", metadata={'audio_url': 'https://storage.googleapis.com/aai-web-samples/langchain_agents_webinar.opus'})]\u001b[0m\u001b[32;1m\u001b[1;3mSure! Here are the main topics discussed in the webinars:\n",
      "\n",
      "- Relationship between the way we operate and think, and building autonomous agents\n",
      "- Task prioritization for building better agents\n",
      "- Safety concerns in AI, including security and potential misuse of capabilities\n",
      "- Assuaging concerns about security and the impact of web scraping and API hitting bots\n",
      "- Panelists' perspectives on addressing safety concerns\n",
      "- Langchain and its relevance to the discussion\n",
      "- Appreciation for panelists' work and sharing knowledge with the community\n",
      "- Challenges in the initial generation of AI models and the importance of iterative improvement\n",
      "\n",
      "Please note that these topics are a summary of the main points covered in the webinars.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe term \"agents\" refers to software programs or systems that are designed to perform specific tasks or functions autonomously. These agents can range from simple rule-based systems to more complex artificial intelligence (AI) systems. Agents are typically built to interact with users or other systems, gather information, make decisions, and perform actions based on predefined rules or learned behaviors.\n",
      "\n",
      "In the context of the audio, the term \"agents\" specifically refers to conversational agents or dialogue systems. These agents are designed to engage in conversations with users, understand their queries or requests, and provide relevant responses or actions. The discussions in the audio explore various architectures, frameworks, and techniques for building and improving these conversational agents, including topics like self-improvement, efficient querying, model generalization, and memory access.\n",
      "\n",
      "Overall, agents are software entities that can perform tasks autonomously, and in the context of the audio, the focus is on conversational agents or dialogue systems.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYou're welcome! I'm glad I could provide you with the information you needed. If you have any more questions, feel free to ask. I'm here to help!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def predict(message, history):\n",
    "    history = []\n",
    "    for human, ai in history:\n",
    "        history.append(HumanMessage(content=human))\n",
    "        history.append(AIMessage(content=ai))\n",
    "    history.append(HumanMessage(content=message))\n",
    "    response = agent_executor.invoke({\"input\": message})\n",
    "    return response[\"output\"]\n",
    "    \n",
    "gr.ChatInterface(predict).queue().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6c9a1-03ad-4649-bfaa-972a24a448c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
